#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Anki APKG extractor.

Supports:
- 2.1b packages (collection.anki21b + zstd + protobuf media/templates)
- 2.1 legacy packages (collection.anki21 + JSON media)
- 2.0 early packages (collection.anki2 + JSON media)

Environment:
- Python >= 3.8
- Required: standard library only
- Optional (required for 2.1b):
    zstandard >= 0.19
    protobuf >= 4.21 (google.protobuf)
  Without these, 2.1/2.0 packages are still supported.

Usage:
- Run directly: scan *.apkg in the script directory and extract.
- CLI (optional):
    python apkg_extract.py [--apkg PATH ...] [--yes]
  --apkg  one or more .apkg paths; if omitted, uses *.apkg in the script dir
  --yes   skip overwrite prompt (or set env ANKI_EXTRACT_FORCE=1)

Output:
- <apkg-basename>/
    notes.csv
    medias/      reconstructed media files
    templates/   each notetype's CSS and the used template HTML

Notes:
- CSV is UTF-8 without BOM; the first lines contain Anki import metadata.
"""

import argparse
import csv
import glob
import io
import json
import os
import re
import shutil
import sqlite3
import sys
import zipfile
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

# Optional dependencies
try:
    import zstandard  # type: ignore
except Exception:
    zstandard = None  # type: ignore[assignment]

# Optional protobufs for 2.1b templates/media parsing
# -------- Embedded Anki protobuf modules --------
NOTETYPES_PB2_SRC = r'''# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: anki/notetypes.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from anki import generic_pb2 as anki_dot_generic__pb2
from anki import collection_pb2 as anki_dot_collection__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x14\x61nki/notetypes.proto\x12\x0e\x61nki.notetypes\x1a\x12\x61nki/generic.proto\x1a\x15\x61nki/collection.proto\"\x1a\n\nNotetypeId\x12\x0c\n\x04ntid\x18\x01 \x01(\x03\"\xa1\x0c\n\x08Notetype\x12\n\n\x02id\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x12\n\nmtime_secs\x18\x03 \x01(\x03\x12\x0b\n\x03usn\x18\x04 \x01(\x11\x12/\n\x06\x63onfig\x18\x07 \x01(\x0b\x32\x1f.anki.notetypes.Notetype.Config\x12.\n\x06\x66ields\x18\x08 \x03(\x0b\x32\x1e.anki.notetypes.Notetype.Field\x12\x34\n\ttemplates\x18\t \x03(\x0b\x32!.anki.notetypes.Notetype.Template\x1a\xdb\x04\n\x06\x43onfig\x12\x32\n\x04kind\x18\x01 \x01(\x0e\x32$.anki.notetypes.Notetype.Config.Kind\x12\x16\n\x0esort_field_idx\x18\x02 \x01(\r\x12\x0b\n\x03\x63ss\x18\x03 \x01(\t\x12\x1d\n\x15target_deck_id_unused\x18\x04 \x01(\x03\x12\x11\n\tlatex_pre\x18\x05 \x01(\t\x12\x12\n\nlatex_post\x18\x06 \x01(\t\x12\x11\n\tlatex_svg\x18\x07 \x01(\x08\x12=\n\x04reqs\x18\x08 \x03(\x0b\x32/.anki.notetypes.Notetype.Config.CardRequirement\x12L\n\x13original_stock_kind\x18\t \x01(\x0e\x32/.anki.notetypes.StockNotetype.OriginalStockKind\x12\x18\n\x0boriginal_id\x18\n \x01(\x03H\x00\x88\x01\x01\x12\x0e\n\x05other\x18\xff\x01 \x01(\x0c\x1a\xae\x01\n\x0f\x43\x61rdRequirement\x12\x10\n\x08\x63\x61rd_ord\x18\x01 \x01(\r\x12\x42\n\x04kind\x18\x02 \x01(\x0e\x32\x34.anki.notetypes.Notetype.Config.CardRequirement.Kind\x12\x12\n\nfield_ords\x18\x03 \x03(\r\"1\n\x04Kind\x12\r\n\tKIND_NONE\x10\x00\x12\x0c\n\x08KIND_ANY\x10\x01\x12\x0c\n\x08KIND_ALL\x10\x02\"\'\n\x04Kind\x12\x0f\n\x0bKIND_NORMAL\x10\x00\x12\x0e\n\nKIND_CLOZE\x10\x01\x42\x0e\n\x0c_original_id\x1a\xf2\x02\n\x05\x46ield\x12!\n\x03ord\x18\x01 \x01(\x0b\x32\x14.anki.generic.UInt32\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x35\n\x06\x63onfig\x18\x05 \x01(\x0b\x32%.anki.notetypes.Notetype.Field.Config\x1a\x80\x02\n\x06\x43onfig\x12\x0e\n\x06sticky\x18\x01 \x01(\x08\x12\x0b\n\x03rtl\x18\x02 \x01(\x08\x12\x11\n\tfont_name\x18\x03 \x01(\t\x12\x11\n\tfont_size\x18\x04 \x01(\r\x12\x13\n\x0b\x64\x65scription\x18\x05 \x01(\t\x12\x12\n\nplain_text\x18\x06 \x01(\x08\x12\x11\n\tcollapsed\x18\x07 \x01(\x08\x12\x1b\n\x13\x65xclude_from_search\x18\x08 \x01(\x08\x12\x0f\n\x02id\x18\t \x01(\x03H\x00\x88\x01\x01\x12\x10\n\x03tag\x18\n \x01(\rH\x01\x88\x01\x01\x12\x18\n\x10prevent_deletion\x18\x0b \x01(\x08\x12\x0e\n\x05other\x18\xff\x01 \x01(\x0c\x42\x05\n\x03_idB\x06\n\x04_tag\x1a\xef\x02\n\x08Template\x12!\n\x03ord\x18\x01 \x01(\x0b\x32\x14.anki.generic.UInt32\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x12\n\nmtime_secs\x18\x03 \x01(\x03\x12\x0b\n\x03usn\x18\x04 \x01(\x11\x12\x38\n\x06\x63onfig\x18\x05 \x01(\x0b\x32(.anki.notetypes.Notetype.Template.Config\x1a\xd6\x01\n\x06\x43onfig\x12\x10\n\x08q_format\x18\x01 \x01(\t\x12\x10\n\x08\x61_format\x18\x02 \x01(\t\x12\x18\n\x10q_format_browser\x18\x03 \x01(\t\x12\x18\n\x10\x61_format_browser\x18\x04 \x01(\t\x12\x16\n\x0etarget_deck_id\x18\x05 \x01(\x03\x12\x19\n\x11\x62rowser_font_name\x18\x06 \x01(\t\x12\x19\n\x11\x62rowser_font_size\x18\x07 \x01(\r\x12\x0f\n\x02id\x18\x08 \x01(\x03H\x00\x88\x01\x01\x12\x0e\n\x05other\x18\xff\x01 \x01(\x0c\x42\x05\n\x03_id\"_\n\x1a\x41\x64\x64OrUpdateNotetypeRequest\x12\x0c\n\x04json\x18\x01 \x01(\x0c\x12\x1e\n\x16preserve_usn_and_mtime\x18\x02 \x01(\x08\x12\x13\n\x0bskip_checks\x18\x03 \x01(\x08\"@\n\x1bUpdateNotetypeLegacyRequest\x12\x0c\n\x04json\x18\x01 \x01(\x0c\x12\x13\n\x0bskip_checks\x18\x02 \x01(\x08\"\xfb\x03\n\rStockNotetype\x12\x30\n\x04kind\x18\x01 \x01(\x0e\x32\".anki.notetypes.StockNotetype.Kind\"\x96\x01\n\x04Kind\x12\x0e\n\nKIND_BASIC\x10\x00\x12\x1b\n\x17KIND_BASIC_AND_REVERSED\x10\x01\x12 \n\x1cKIND_BASIC_OPTIONAL_REVERSED\x10\x02\x12\x15\n\x11KIND_BASIC_TYPING\x10\x03\x12\x0e\n\nKIND_CLOZE\x10\x04\x12\x18\n\x14KIND_IMAGE_OCCLUSION\x10\x05\"\x9e\x02\n\x11OriginalStockKind\x12\x1f\n\x1bORIGINAL_STOCK_KIND_UNKNOWN\x10\x00\x12\x1d\n\x19ORIGINAL_STOCK_KIND_BASIC\x10\x01\x12*\n&ORIGINAL_STOCK_KIND_BASIC_AND_REVERSED\x10\x02\x12/\n+ORIGINAL_STOCK_KIND_BASIC_OPTIONAL_REVERSED\x10\x03\x12$\n ORIGINAL_STOCK_KIND_BASIC_TYPING\x10\x04\x12\x1d\n\x19ORIGINAL_STOCK_KIND_CLOZE\x10\x05\x12\'\n#ORIGINAL_STOCK_KIND_IMAGE_OCCLUSION\x10\x06\"@\n\rNotetypeNames\x12/\n\x07\x65ntries\x18\x01 \x03(\x0b\x32\x1e.anki.notetypes.NotetypeNameId\"L\n\x11NotetypeUseCounts\x12\x37\n\x07\x65ntries\x18\x01 \x03(\x0b\x32&.anki.notetypes.NotetypeNameIdUseCount\"*\n\x0eNotetypeNameId\x12\n\n\x02id\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\t\"E\n\x16NotetypeNameIdUseCount\x12\n\n\x02id\x18\x01 \x01(\x03\x12\x0c\n\x04name\x18\x02 \x01(\t\x12\x11\n\tuse_count\x18\x03 \x01(\r\"1\n\x16GetAuxConfigKeyRequest\x12\n\n\x02id\x18\x01 \x01(\x03\x12\x0b\n\x03key\x18\x02 \x01(\t\"X\n\x1eGetAuxTemplateConfigKeyRequest\x12\x13\n\x0bnotetype_id\x18\x01 \x01(\x03\x12\x14\n\x0c\x63\x61rd_ordinal\x18\x02 \x01(\r\x12\x0b\n\x03key\x18\x03 \x01(\t\"P\n\x1cGetChangeNotetypeInfoRequest\x12\x17\n\x0fold_notetype_id\x18\x01 \x01(\x03\x12\x17\n\x0fnew_notetype_id\x18\x02 \x01(\x03\"\xcb\x01\n\x15\x43hangeNotetypeRequest\x12\x10\n\x08note_ids\x18\x01 \x03(\x03\x12\x12\n\nnew_fields\x18\x02 \x03(\x05\x12\x15\n\rnew_templates\x18\x03 \x03(\x05\x12\x17\n\x0fold_notetype_id\x18\x04 \x01(\x03\x12\x17\n\x0fnew_notetype_id\x18\x05 \x01(\x03\x12\x16\n\x0e\x63urrent_schema\x18\x06 \x01(\x03\x12\x19\n\x11old_notetype_name\x18\x07 \x01(\t\x12\x10\n\x08is_cloze\x18\x08 \x01(\x08\"\xcf\x01\n\x12\x43hangeNotetypeInfo\x12\x17\n\x0fold_field_names\x18\x01 \x03(\t\x12\x1a\n\x12old_template_names\x18\x02 \x03(\t\x12\x17\n\x0fnew_field_names\x18\x03 \x03(\t\x12\x1a\n\x12new_template_names\x18\x04 \x03(\t\x12\x34\n\x05input\x18\x05 \x01(\x0b\x32%.anki.notetypes.ChangeNotetypeRequest\x12\x19\n\x11old_notetype_name\x18\x06 \x01(\t\"\x9c\x01\n\x1dRestoreNotetypeToStockRequest\x12/\n\x0bnotetype_id\x18\x01 \x01(\x0b\x32\x1a.anki.notetypes.NotetypeId\x12;\n\nforce_kind\x18\x02 \x01(\x0e\x32\".anki.notetypes.StockNotetype.KindH\x00\x88\x01\x01\x42\r\n\x0b_force_kind*\xc8\x01\n\x13ImageOcclusionField\x12$\n IMAGE_OCCLUSION_FIELD_OCCLUSIONS\x10\x00\x12\x1f\n\x1bIMAGE_OCCLUSION_FIELD_IMAGE\x10\x01\x12 \n\x1cIMAGE_OCCLUSION_FIELD_HEADER\x10\x02\x12$\n IMAGE_OCCLUSION_FIELD_BACK_EXTRA\x10\x03\x12\"\n\x1eIMAGE_OCCLUSION_FIELD_COMMENTS\x10\x04*>\n\nClozeField\x12\x14\n\x10\x43LOZE_FIELD_TEXT\x10\x00\x12\x1a\n\x16\x43LOZE_FIELD_BACK_EXTRA\x10\x01\x32\xdd\x0b\n\x10NotetypesService\x12I\n\x0b\x41\x64\x64Notetype\x12\x18.anki.notetypes.Notetype\x1a .anki.collection.OpChangesWithId\x12\x46\n\x0eUpdateNotetype\x12\x18.anki.notetypes.Notetype\x1a\x1a.anki.collection.OpChanges\x12I\n\x11\x41\x64\x64NotetypeLegacy\x12\x12.anki.generic.Json\x1a .anki.collection.OpChangesWithId\x12_\n\x14UpdateNotetypeLegacy\x12+.anki.notetypes.UpdateNotetypeLegacyRequest\x1a\x1a.anki.collection.OpChanges\x12]\n\x13\x41\x64\x64OrUpdateNotetype\x12*.anki.notetypes.AddOrUpdateNotetypeRequest\x1a\x1a.anki.notetypes.NotetypeId\x12K\n\x16GetStockNotetypeLegacy\x12\x1d.anki.notetypes.StockNotetype\x1a\x12.anki.generic.Json\x12\x43\n\x0bGetNotetype\x12\x1a.anki.notetypes.NotetypeId\x1a\x18.anki.notetypes.Notetype\x12\x43\n\x11GetNotetypeLegacy\x12\x1a.anki.notetypes.NotetypeId\x1a\x12.anki.generic.Json\x12\x46\n\x10GetNotetypeNames\x12\x13.anki.generic.Empty\x1a\x1d.anki.notetypes.NotetypeNames\x12S\n\x19GetNotetypeNamesAndCounts\x12\x13.anki.generic.Empty\x1a!.anki.notetypes.NotetypeUseCounts\x12G\n\x13GetNotetypeIdByName\x12\x14.anki.generic.String\x1a\x1a.anki.notetypes.NotetypeId\x12H\n\x0eRemoveNotetype\x12\x1a.anki.notetypes.NotetypeId\x1a\x1a.anki.collection.OpChanges\x12W\n\x17GetAuxNotetypeConfigKey\x12&.anki.notetypes.GetAuxConfigKeyRequest\x1a\x14.anki.generic.String\x12_\n\x17GetAuxTemplateConfigKey\x12..anki.notetypes.GetAuxTemplateConfigKeyRequest\x1a\x14.anki.generic.String\x12i\n\x15GetChangeNotetypeInfo\x12,.anki.notetypes.GetChangeNotetypeInfoRequest\x1a\".anki.notetypes.ChangeNotetypeInfo\x12S\n\x0e\x43hangeNotetype\x12%.anki.notetypes.ChangeNotetypeRequest\x1a\x1a.anki.collection.OpChanges\x12\x45\n\rGetFieldNames\x12\x1a.anki.notetypes.NotetypeId\x1a\x18.anki.generic.StringList\x12\x63\n\x16RestoreNotetypeToStock\x12-.anki.notetypes.RestoreNotetypeToStockRequest\x1a\x1a.anki.collection.OpChanges2\x19\n\x17\x42\x61\x63kendNotetypesServiceB\x02P\x01\x62\x06proto3')
...
# (protobuf source string continues unchanged)
'''

IMPORT_EXPORT_PB2_SRC = r'''# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: anki/import_export.proto
"""Generated protocol buffer code."""
...
# (protobuf source string continues unchanged)
'''

# Build protobuf modules if available in the environment;
# otherwise fall back to embedded definitions above.
PROTO_AVAILABLE = True
try:
    from anki import import_export_pb2, notetypes_pb2  # type: ignore
    from google.protobuf.json_format import MessageToDict  # type: ignore
except Exception:
    try:
        import types
        # Ensure 'anki' package placeholder exists
        if 'anki' not in sys.modules:
            anki_pkg = types.ModuleType('anki')
            anki_pkg.__path__ = []
            sys.modules['anki'] = anki_pkg
        # Load embedded modules
        mod_nt = types.ModuleType('anki.notetypes_pb2')
        mod_ie = types.ModuleType('anki.import_export_pb2')
        mod_nt.__package__ = 'anki'
        mod_ie.__package__ = 'anki'
        exec(NOTETYPES_PB2_SRC, mod_nt.__dict__)
        exec(IMPORT_EXPORT_PB2_SRC, mod_ie.__dict__)
        sys.modules['anki.notetypes_pb2'] = mod_nt
        sys.modules['anki.import_export_pb2'] = mod_ie
        from anki import import_export_pb2, notetypes_pb2  # type: ignore
        from google.protobuf.json_format import MessageToDict  # type: ignore
    except Exception:
        PROTO_AVAILABLE = False

# Paths and constants
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
NOTES_CSV_NAME = "notes.csv"
TEMPLATES_DIR_NAME = "templates"
MEDIA_DIR_NAME = "medias"
CSV_DELIMITER = ','

# ----------------------- Utility -----------------------

def safe_filename(name: str) -> str:
    """Return a filesystem-safe name."""
    return re.sub(r'[\\/:*?"<>|]', '_', (name or "").strip())

def ensure_dir(path: str) -> None:
    """Create directory if missing."""
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)

def safe_zip_extractall(zf: zipfile.ZipFile, out_dir: str) -> List[str]:
    """Prevent Zip Slip; extract after verifying members stay under out_dir. Return member names."""
    names = zf.namelist()
    out_abs = os.path.abspath(out_dir)
    for zi in zf.infolist():
        dest = os.path.abspath(os.path.join(out_dir, zi.filename))
        if not (dest == out_abs or dest.startswith(out_abs + os.sep)):
            raise RuntimeError(f"Unsafe member in zip: {zi.filename}")
    zf.extractall(out_dir)
    return names

def zstd_decompress_bytes(data: bytes) -> bytes:
    """Decompress zstd bytes using optional dependency."""
    if not zstandard:
        raise RuntimeError("zstandard module not installed")
    dctx = zstandard.ZstdDecompressor()
    with dctx.stream_reader(io.BytesIO(data)) as r:
        return r.read()

def zstd_decompress_file_inplace(path: str) -> None:
    """In-place zstd decompression for a file."""
    with open(path, 'rb') as f:
        data = f.read()
    decomp = zstd_decompress_bytes(data)
    with open(path, 'wb') as f:
        f.write(decomp)

# ----------------------- DB helper -----------------------

class SQLiteDB:
    """Tiny helper wrapper around sqlite3 for schema inspection and simple queries."""
    def __init__(self, path: str):
        self.path = path
        self.conn: Optional[sqlite3.Connection] = None
        self._tables: Optional[List[str]] = None
        self._columns: Dict[str, List[str]] = {}

    def __enter__(self) -> "SQLiteDB":
        self.open()
        return self

    def __exit__(self, exc_type, exc, tb):
        self.close()

    def open(self):
        self.conn = sqlite3.connect(self.path)

    def close(self):
        if self.conn:
            self.conn.close()
            self.conn = None

    def _execute(self, query: str, args: Optional[Sequence]=None) -> List[Tuple]:
        cur = self.conn.cursor()  # type: ignore[union-attr]
        cur.execute(query, args or [])
        return cur.fetchall()

    def tables(self) -> List[str]:
        if self._tables is None:
            rows = self._execute("SELECT name FROM sqlite_master WHERE type='table';")
            self._tables = [r[0] for r in rows]
        return self._tables

    def columns(self, table: str) -> List[str]:
        if table in self._columns:
            return self._columns[table]
        rows = self._execute(f"PRAGMA table_info({table});")
        cols = [r[1] for r in rows]
        self._columns[table] = cols
        return cols

    def rows(self, table: str) -> List[Dict[str, object]]:
        rows = self._execute(f"SELECT * FROM {table};")
        cols = self.columns(table)
        return [{cols[i]: row[i] for i in range(len(cols))} for row in rows]

    def execute(self, query: str, args: Optional[Sequence]=None) -> List[Tuple]:
        return self._execute(query, args)

# ----------------------- 2.1b Protobuf helpers -----------------------

def pb_get_notetype_config(data: Optional[bytes]) -> Dict:
    """Decode notetype config message -> dict. Empty dict if unavailable."""
    if not PROTO_AVAILABLE or not data:
        return {}
    obj = notetypes_pb2.Notetype().Config()
    obj.ParseFromString(data)
    return MessageToDict(obj)

def pb_get_template_config(data: Optional[bytes]) -> Dict:
    """Decode template config message -> dict. Empty dict if unavailable."""
    if not PROTO_AVAILABLE or not data:
        return {}
    obj = notetypes_pb2.Notetype().Template().Config()
    obj.ParseFromString(data)
    return MessageToDict(obj)

def pb_parse_media_entries(data: Optional[bytes]) -> Dict[int, Dict[str, object]]:
    """Parse MediaEntries and return an index->info mapping."""
    if not PROTO_AVAILABLE or not data:
        return {}
    media_entries = import_export_pb2.MediaEntries()
    media_entries.ParseFromString(data)
    entries: Dict[int, Dict[str, object]] = {}
    for idx, entry in enumerate(media_entries.entries):
        entries[idx] = {
            "name": entry.name,
            "size": entry.size,
            "sha1": entry.sha1.hex(),
        }
    return entries

# ----------------------- Extraction paths -----------------------

def extract_apkg(apkg_path: str, out_dir: str) -> List[str]:
    """Extract the .apkg into out_dir and return contained names."""
    ensure_dir(out_dir)
    with zipfile.ZipFile(apkg_path, 'r') as z:
        return safe_zip_extractall(z, out_dir)

# ----------------------- Media handlers -----------------------

def process_media_json(base_dir: str, media_dir_name: str = MEDIA_DIR_NAME) -> None:
    """Handle legacy media mapping (media or media.json). Move numbered files into medias/."""
    mapping_path: Optional[str] = None
    for cand in ("media", "media.json"):
        p = os.path.join(base_dir, cand)
        if os.path.exists(p):
            mapping_path = p
            break
    if not mapping_path:
        print("Warn: media mapping file missing")
        return
    try:
        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
    except Exception:
        # Some exporters produce JSON with minor deviations; try again via raw read
        with open(mapping_path, "r", encoding="utf-8") as f:
            txt = f.read()
        mapping = json.loads(txt)
    media_dir = os.path.join(base_dir, media_dir_name)
    ensure_dir(media_dir)
    for key, filename in mapping.items():
        src = os.path.join(base_dir, str(key))
        if os.path.exists(src):
            dst = os.path.join(media_dir, filename)
            ensure_dir(os.path.dirname(dst))
            try:
                shutil.move(src, dst)
            except Exception as e:
                print(f"Warn: cannot move {src} -> {dst}: {e}")

def process_media_pb_zstd(base_dir: str, media_dir_name: str = MEDIA_DIR_NAME) -> None:
    """Handle 2.1b media: media index is protobuf+zstd; individual files are zstd-compressed."""
    if not PROTO_AVAILABLE:
        print("Warn: protobuf modules unavailable. Skipping media rename for 21b.")
        return
    media_pb_path = os.path.join(base_dir, "media")
    with open(media_pb_path, "rb") as f:
        comp = f.read()
    media_bytes = zstd_decompress_bytes(comp)
    entries = pb_parse_media_entries(media_bytes)
    media_dir = os.path.join(base_dir, media_dir_name)
    ensure_dir(media_dir)
    for key, info in entries.items():
        num_name = str(key)
        num_path = os.path.join(base_dir, num_name)
        if not os.path.exists(num_path):
            continue
        try:
            with open(num_path, "rb") as f:
                data = f.read()
            data = zstd_decompress_bytes(data)
        except Exception as e:
            print(f"Warn: failed to decompress media {num_name}: {e}")
            continue
        dst = os.path.join(media_dir, info.get("name") or num_name)
        ensure_dir(os.path.dirname(dst))
        with open(dst, "wb") as f:
            f.write(data)
        try:
            os.remove(num_path)
        except Exception:
            pass

# ----------------------- Template writers -----------------------

def _detect_note_nt_column(db: SQLiteDB) -> str:
    """Detect the notetype id column name in notes table across schema variants."""
    note_cols = db.columns("notes")
    if "ntid" in note_cols:
        return "ntid"
    if "mid" in note_cols:
        return "mid"
    if "model_id" in note_cols:
        return "model_id"
    raise RuntimeError("notes table lacks ntid/mid/model_id column")

def write_templates_from_21b(db_path: str, out_dir: str) -> None:
    """Export CSS and only the used templates for each notetype in 2.1b schema."""
    if not PROTO_AVAILABLE:
        print("Warn: protobuf unavailable. Skipping templates for 21b.")
        return
    with SQLiteDB(db_path) as db:
        if "notetypes" not in db.tables() or "templates" not in db.tables():
            print("Warn: templates tables missing")
            return

        # Column detection to handle multiple 2.1b schemas
        nt_cols = set(db.columns("notetypes"))
        tpl_cols = set(db.columns("templates"))

        nt_id_col  = "id" if "id" in nt_cols else None
        nt_name_col = "name" if "name" in nt_cols else None
        nt_cfg_col = "config" if "config" in nt_cols else None
        if not (nt_id_col and nt_cfg_col):
            print("Warn: notetypes lacks id/config")
            return

        tpl_nt_col = "ntid" if "ntid" in tpl_cols else ("notetype_id" if "notetype_id" in tpl_cols else None)
        tpl_name_col = "name" if "name" in tpl_cols else None
        tpl_cfg_col = "config" if "config" in tpl_cols else None
        if not (tpl_nt_col and tpl_name_col and tpl_cfg_col):
            print("Warn: templates lacks ntid/name/config")
            return

        templates_dir = os.path.join(out_dir, TEMPLATES_DIR_NAME)
        ensure_dir(templates_dir)

        # Export CSS and all templates for each notetype (consistent with main_new.py)
        nt_rows = db.execute(f"SELECT {nt_id_col}, {nt_name_col if nt_name_col else 'NULL'}, {nt_cfg_col} FROM notetypes")
        for ntid, ntname, ntcfg in nt_rows:
            nt_info = pb_get_notetype_config(ntcfg or b"")
            if not ntname:
                ntname = nt_info.get("name", f"Notetype_{ntid}")
            css = nt_info.get("css", "")

            nt_name_sf = safe_filename(ntname)
            css_path = os.path.join(templates_dir, f"{nt_name_sf}-styles.css")
            with open(css_path, "w", encoding="utf-8") as f:
                f.write(css)

            # Export all templates for this notetype
            trows = db.execute(f"SELECT {tpl_name_col}, {tpl_cfg_col} FROM templates WHERE {tpl_nt_col}=?", [ntid])
            for tplname, tplcfg in trows:
                tpl_info = pb_get_template_config(tplcfg or b"")
                q = tpl_info.get("qFormat", "")
                a = tpl_info.get("aFormat", "")
                tpl_filename = f"{nt_name_sf}-{safe_filename(tplname or 'template')}.html"
                with open(os.path.join(templates_dir, tpl_filename), "w", encoding="utf-8") as f:
                    # Export q + single newline + a (consistent with main_new.py)
                    f.write(q + "\n" + a)

def write_templates_from_legacy(db_path: str, out_dir: str) -> None:
    """Export CSS and templates from legacy 'col.models' JSON. Only for used models if possible."""
    with SQLiteDB(db_path) as db:
        if "col" not in db.tables():
            print("Warn: col table missing")
            return
        rows = db.execute("SELECT models FROM col")
        if not rows:
            return
        models = json.loads(rows[0][0])
        templates_dir = os.path.join(out_dir, TEMPLATES_DIR_NAME)
        ensure_dir(templates_dir)
        note_cols = db.columns("notes")
        used_mids = set()
        if "mid" in note_cols:
            used_mids = {r[0] for r in db.execute("SELECT DISTINCT mid FROM notes")}
        for mid, model in models.items():
            mid_int = int(mid)
            if used_mids and mid_int not in used_mids:
                continue
            model_name = model.get("name", f"Model_{mid}")
            css = model.get("css", "")
            css_path = os.path.join(templates_dir, f"{safe_filename(model_name)}-styles.css")
            with open(css_path, "w", encoding="utf-8") as f:
                f.write(css)
            for t in model.get("tmpls", []) or []:
                tpl_name = t.get("name", "UnnamedTemplate")
                qfmt = t.get("qfmt", "")
                afmt = t.get("afmt", "")
                tpl_file = f"{safe_filename(model_name)}-{safe_filename(tpl_name)}.html"
                with open(os.path.join(templates_dir, tpl_file), "w", encoding="utf-8") as f:
                    f.write(qfmt + "\n\n" + afmt + "\n")

# ----------------------- Notes CSV -----------------------

def query_notes_with_deck(db_path: str) -> List[Tuple]:
    """Return (note_id, fields, tags, min_deck_id) for notes that have cards."""
    with SQLiteDB(db_path) as db:
        query = """
        SELECT notes.id, notes.flds, notes.tags, MIN(cards.did)
        FROM notes
        JOIN cards ON cards.nid = notes.id
        GROUP BY notes.id
        """
        rows = db.execute(query)
        return rows

def get_deck_map_from_col(db_path: str) -> Dict[str, str]:
    """Legacy 'col.decks' JSON -> {deck_id: deck_name}."""
    with SQLiteDB(db_path) as db:
        rows = db.execute("SELECT decks FROM col")
        if not rows:
            return {}
        decks = json.loads(rows[0][0])
        return {str(k): v.get("name", "UnknownDeck") for k, v in decks.items()}

def get_deck_map_from_decks_table(db_path: str) -> Dict[str, str]:
    """2.1b decks table -> {deck_id: deck_name} when available."""
    with SQLiteDB(db_path) as db:
        if "decks" not in db.tables():
            return {}
        rows = db.rows("decks")
        return {str(r.get("id")): r.get("name", "UnknownDeck") for r in rows}

def write_notes_csv(out_dir: str, deck_map: Dict[str, str], rows: List[Tuple]) -> None:
    """
    Write notes.csv with Anki import hints.
    - Fields are joined by U+001F in Anki; split and right-pad rows.
    - First column is deck name; last column is tags.
    """
    notes_csv_path = os.path.join(out_dir, NOTES_CSV_NAME)
    max_fields = 0
    parsed: List[Tuple[List[str], str, str]] = []
    for note_id, flds, tags, did in rows:
        fields = (flds or "").split("\x1f")  # Anki field separator
        max_fields = max(max_fields, len(fields))
        parsed.append((fields, tags or "", str(did) if did is not None else ""))
    deck_col = 1
    tags_col = max_fields + 2
    with open(notes_csv_path, "w", newline="", encoding="utf-8") as f:
        # Anki CSV import metadata
        f.write("#separator:comma\n")
        f.write("#html:true\n")
        f.write(f"#deck column:{deck_col}\n")
        f.write(f"#tags column:{tags_col}\n")
        w = csv.writer(f, delimiter=CSV_DELIMITER)
        for fields, tags, did in parsed:
            deck_name = deck_map.get(str(did), "UnknownDeck")
            # Keep deck hierarchy; replace Anki's field separator if present
            deck_name = deck_name.encode("utf-8").replace(b"\x1f", b"::").decode("utf-8")
            row = [deck_name] + fields + ([""] * (max_fields - len(fields))) + [tags.strip()]
            w.writerow(row)

# ----------------------- Main flow -----------------------

def clean_unrelated(out_dir: str, keep: set = None) -> None:
    """Remove files produced by extraction but unrelated to final output."""
    if keep is None:
        keep = {NOTES_CSV_NAME, MEDIA_DIR_NAME, TEMPLATES_DIR_NAME}
    for item in os.listdir(out_dir):
        if item not in keep:
            p = os.path.join(out_dir, item)
            if os.path.isdir(p):
                shutil.rmtree(p, ignore_errors=True)
            else:
                try:
                    os.remove(p)
                except Exception:
                    pass

def process_apkg(apkg_path: str, force: bool = False) -> None:
    """Process a single .apkg into <basename>/ with notes.csv, medias/, templates/."""
    apkg_dir = os.path.join(os.path.dirname(apkg_path), os.path.splitext(os.path.basename(apkg_path))[0])
    if os.path.exists(apkg_dir):
        if force or os.environ.get("ANKI_EXTRACT_FORCE", ""):
            shutil.rmtree(apkg_dir, ignore_errors=True)
        else:
            ans = input(f"{apkg_dir} exists. Delete and re-extract? (y/n) ")
            if ans.lower() in ("y", "yes"):
                shutil.rmtree(apkg_dir, ignore_errors=True)
            else:
                print(f"Skip {apkg_path}")
                return
    namelist = extract_apkg(apkg_path, apkg_dir)
    has21b = "collection.anki21b" in namelist
    has21  = "collection.anki21" in namelist
    has20  = "collection.anki2"  in namelist

    if has21b:
        if not zstandard or not PROTO_AVAILABLE:
            print("Error: 21b package needs zstandard and protobuf modules.")
            return
        coll_path = os.path.join(apkg_dir, "collection.anki21b")
        zstd_decompress_file_inplace(coll_path)
        process_media_pb_zstd(apkg_dir, MEDIA_DIR_NAME)
        rows = query_notes_with_deck(coll_path)
        deck_map = get_deck_map_from_decks_table(coll_path) or get_deck_map_from_col(coll_path)
        write_notes_csv(apkg_dir, deck_map, rows)
        write_templates_from_21b(coll_path, apkg_dir)
        clean_unrelated(apkg_dir)
        print(f"Processed {apkg_path} -> {apkg_dir}")
        return

    if has21 or has20:
        db_name = "collection.anki21" if has21 else "collection.anki2"
        db_path = os.path.join(apkg_dir, db_name)
        process_media_json(apkg_dir, MEDIA_DIR_NAME)
        rows = query_notes_with_deck(db_path)
        deck_map = get_deck_map_from_col(db_path)
        write_notes_csv(apkg_dir, deck_map, rows)
        write_templates_from_legacy(db_path, apkg_dir)
        clean_unrelated(apkg_dir)
        print(f"Processed {apkg_path} -> {apkg_dir}")
        return

    print("Unsupported APKG: no collection.anki21b, collection.anki21, or collection.anki2 found.")

def _parse_args() -> argparse.Namespace:
    """Parse CLI arguments."""
    p = argparse.ArgumentParser(description="Extract Anki .apkg to csv/templates/medias.")
    p.add_argument("--apkg", nargs="*", help="One or more .apkg paths. Default: scan SCRIPT_DIR/*.apkg")
    p.add_argument("--yes", action="store_true", help="Overwrite without prompt")
    return p.parse_args()

def main():
    """Entry point."""
    args = _parse_args()
    apkg_files: List[str]
    if args.apkg:
        apkg_files = []
        for a in args.apkg:
            if os.path.isdir(a):
                apkg_files.extend(sorted(glob.glob(os.path.join(a, "*.apkg"))))
            else:
                if a.lower().endswith(".apkg") and os.path.exists(a):
                    apkg_files.append(a)
    else:
        apkg_files = sorted(glob.glob(os.path.join(SCRIPT_DIR, "*.apkg")))
    if not apkg_files:
        print("No APKG files found.")
        return
    for apkg in apkg_files:
        process_apkg(apkg, force=bool(args.yes))

if __name__ == "__main__":
    main()
